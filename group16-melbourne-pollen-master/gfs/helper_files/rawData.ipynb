{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use this file when connection times-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygrib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from mpl_toolkits.basemap import Basemap, addcyclic\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup to download grib files from RDA archive\n",
    "\n",
    "try:\n",
    "    import getpass\n",
    "    input = getpass.getpass\n",
    "except:\n",
    "    try:\n",
    "        input = raw_input\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user specific data to login\n",
    "\n",
    "pswd = input('password:')\n",
    "values = {'email' : 'nandlalm@student.unimelb.edu.au', 'passwd' : pswd, 'action' : 'login'}\n",
    "login_url = 'https://rda.ucar.edu/cgi-bin/login'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = requests.post(login_url, data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path on RDA catalogue\n",
    "\n",
    "dspath = 'https://rda.ucar.edu/data/ds083.2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destinantion path\n",
    "\n",
    "save_dir = '/pollensource/gfs_archive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box for Australia\n",
    "\n",
    "min_lat = -43.835\n",
    "max_lat = -9.796    \n",
    "min_lon = 112.500\n",
    "max_lon = 154.688\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAus(file):\n",
    "    lat_filter = (file[\"lat\"] >= min_lat) & (file[\"lat\"] <= max_lat)\n",
    "    lon_filter = (file[\"lon\"] >= min_lon) & (file[\"lon\"] <= max_lon)\n",
    "    filtered = file.loc[lat_filter & lon_filter]\n",
    "    return filtered.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant data from the grib files\n",
    "\n",
    "def getData(filename):\n",
    "\n",
    "    fileAddress = '/pollensource/gfs_archive/' + filename\n",
    "    myfile = pygrib.open(fileAddress)\n",
    "\n",
    "    # get surface level temperature data\n",
    "    grb_temp = myfile.select(shortName='t', typeOfLevel = 'surface')\n",
    "    lat = grb_temp[0].latitudes\n",
    "    lon = grb_temp[0].longitudes\n",
    "    vals = grb_temp[0].values.reshape(lon.shape[0],)\n",
    "    date = grb_temp[0].validDate\n",
    "    \n",
    "    df = pd.DataFrame(None, index=range(65160), columns=['datetime', 'lat', 'lon', 't'])  # range hard-coded for now\n",
    "    df['datetime'] = date\n",
    "    df['lat'] = lat\n",
    "    df['lon'] = lon\n",
    "    df['t'] = vals\n",
    "    df['date'] = pd.to_datetime(df['datetime']).dt.date\n",
    "    \n",
    "    # get 2m level temperature data\n",
    "    grb_temp_2m = myfile.select(shortName='2t')\n",
    "    vals_2t = grb_temp_2m[0].values.reshape(lon.shape[0],)\n",
    "    df['t_2m'] = vals_2t\n",
    "\n",
    "    # get mean sea level pressure\n",
    "    try:\n",
    "        grb_msl = myfile.select(shortName='msl')\n",
    "    except ValueError:\n",
    "        grb_msl = myfile.select(shortName='prmsl')\n",
    "    vals_msl = grb_msl[0].values.reshape(lon.shape[0],)\n",
    "    df['msl'] = vals_msl\n",
    "\n",
    "    # get 10m u component of wind\n",
    "    grb_10u = myfile.select(shortName='10u')\n",
    "    vals_10u = grb_10u[0].values.reshape(lon.shape[0],)\n",
    "    df['u_10m'] = vals_10u\n",
    "\n",
    "    # get 10m v component of wind\n",
    "    grb_10v = myfile.select(shortName='10v')\n",
    "    vals_10v = grb_10v[0].values.reshape(lon.shape[0],)\n",
    "    df['v_10m'] = vals_10v\n",
    "\n",
    "    # get the relative humidity\n",
    "    try:\n",
    "        grb_r = myfile.select(shortName='r', typeOfLevel = 'entireAtmosphere')\n",
    "    except ValueError:\n",
    "        grb_r = myfile.select(shortName='r', typeOfLevel = 'atmosphereSingleLayer')\n",
    "\n",
    "    vals_r = grb_r[0].values.reshape(lon.shape[0],)\n",
    "    df['hum_atmos'] = vals_r\n",
    "\n",
    "    # get precipitable water\n",
    "    grb_pwat = myfile.select(shortName='pwat')\n",
    "    vals_pwat = grb_pwat[0].values.reshape(lon.shape[0],)\n",
    "    df['pwat'] = vals_pwat\n",
    "\n",
    "    data  = filterAus(df)\n",
    "    # print(data)\n",
    "    # data['coord'] = (data['lon'], data['lat'])\n",
    "    data['coord'] = list(zip(data.lon, data.lat))\n",
    "    data.coord = data.coord.astype(str)\n",
    "\n",
    "    data = data.drop(['lat', 'lon'], axis = 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToDb(df):\n",
    "\n",
    "    databse = 'gfs_data'\n",
    "    user = 'gfs'\n",
    "    password = 'pollenmelb'\n",
    "    host=\"127.0.0.1\"\n",
    "\n",
    "    conn_string = f\"postgresql://{user}:{password}@{host}/{databse}\"\n",
    "    engine = create_engine(conn_string)\n",
    "    \n",
    "    table_name = 'weatherraw'\n",
    "    \n",
    "    if_exists = 'append'\n",
    "\n",
    "    #Write the data to postgres\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        df.to_sql(\n",
    "            name=table_name.lower(), \n",
    "            con=con, \n",
    "            if_exists=if_exists,\n",
    "            index = False\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the hourly data and create variables\n",
    "\n",
    "def aggregateData9am(dataf):\n",
    "\n",
    "    grouped_t = dataf.groupby(['coord']).agg({'t': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t.columns = ['t_mean_9am', 't_min_9am', 't_max_9am', 't_sd_9am']\n",
    "    grouped_t = grouped_t.reset_index()\n",
    "\n",
    "    grouped_t_2m = dataf.groupby(['coord']).agg({'t_2m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t_2m.columns = ['t_2m_mean_9am', 't_2m_min_9am', 't_2m_max_9am', 't_2m_sd_9am']\n",
    "    grouped_t_2m = grouped_t_2m.reset_index()\n",
    "    \n",
    "    grouped_msl = dataf.groupby(['coord']).agg({'msl': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_msl.columns = ['msl_mean_9am', 'msl_min_9am', 'msl_max_9am', 'msl_sd_9am']\n",
    "    grouped_msl = grouped_msl.reset_index()\n",
    "\n",
    "    grouped_hum_atmos = dataf.groupby(['coord']).agg({'hum_atmos': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_hum_atmos.columns = ['hum_atmos_mean_9am', 'hum_atmos_min_9am', 'hum_atmos_max_9am', 'hum_atmos_sd_9am']\n",
    "    grouped_hum_atmos = grouped_hum_atmos.reset_index()\n",
    "\n",
    "    grouped_u_10m = dataf.groupby(['coord']).agg({'u_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_u_10m.columns = ['u_10m_mean_9am', 'u_10m_min_9am', 'u_10m_max_9am', 'u_10m_sd_9am']\n",
    "    grouped_u_10m = grouped_u_10m.reset_index()\n",
    "\n",
    "    grouped_v_10m = dataf.groupby(['coord']).agg({'v_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_v_10m.columns = ['v_10m_mean_9am', 'v_10m_min_9am', 'v_10m_max_9am', 'v_10m_sd_9am']\n",
    "    grouped_v_10m = grouped_v_10m.reset_index()\n",
    "\n",
    "    grouped_pwat = dataf.groupby(['coord']).agg({'pwat': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_pwat.columns = ['pwat_mean_9am', 'pwat_min_9am', 'pwat_max_9am', 'pwat_sd_9am']\n",
    "    grouped_pwat = grouped_pwat.reset_index()\n",
    "\n",
    "    agg_data = grouped_t.merge(grouped_t_2m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_msl, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_hum_atmos, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_u_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_v_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_pwat, how = 'left', on = 'coord')\n",
    "\n",
    "    dateval = min(dataf.date.unique())\n",
    "    agg_data['date'] = dateval\n",
    "    \n",
    "\n",
    "\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the hourly data and create variables\n",
    "\n",
    "def aggregateData4pm(dataf):\n",
    "\n",
    "    grouped_t = dataf.groupby(['coord']).agg({'t': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t.columns = ['t_mean_4pm', 't_min_4pm', 't_max_4pm', 't_sd_4pm']\n",
    "    grouped_t = grouped_t.reset_index()\n",
    "\n",
    "    grouped_t_2m = dataf.groupby(['coord']).agg({'t_2m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t_2m.columns = ['t_2m_mean_4pm', 't_2m_min_4pm', 't_2m_max_4pm', 't_2m_sd_4pm']\n",
    "    grouped_t_2m = grouped_t_2m.reset_index()\n",
    "\n",
    "    grouped_msl = dataf.groupby(['coord']).agg({'msl': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_msl.columns = ['msl_mean_4pm', 'msl_min_4pm', 'msl_max_4pm', 'msl_sd_4pm']\n",
    "    grouped_msl = grouped_msl.reset_index()\n",
    "\n",
    "    grouped_hum_atmos = dataf.groupby(['coord']).agg({'hum_atmos': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_hum_atmos.columns = ['hum_atmos_mean_4pm', 'hum_atmos_min_4pm', 'hum_atmos_max_4pm', 'hum_atmos_sd_4pm']\n",
    "    grouped_hum_atmos = grouped_hum_atmos.reset_index()\n",
    "\n",
    "    grouped_u_10m = dataf.groupby(['coord']).agg({'u_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_u_10m.columns = ['u_10m_mean_4pm', 'u_10m_min_4pm', 'u_10m_max_4pm', 'u_10m_sd_4pm']\n",
    "    grouped_u_10m = grouped_u_10m.reset_index()\n",
    "\n",
    "    grouped_v_10m = dataf.groupby(['coord']).agg({'v_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_v_10m.columns = ['v_10m_mean_4pm', 'v_10m_min_4pm', 'v_10m_max_4pm', 'v_10m_sd_4pm']\n",
    "    grouped_v_10m = grouped_v_10m.reset_index()\n",
    "\n",
    "    grouped_pwat = dataf.groupby(['coord']).agg({'pwat': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_pwat.columns = ['pwat_mean_4pm', 'pwat_min_4pm', 'pwat_max_4pm', 'pwat_sd_4pm']\n",
    "    grouped_pwat = grouped_pwat.reset_index()\n",
    "\n",
    "\n",
    "    agg_data = grouped_t.merge(grouped_t_2m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_msl, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_hum_atmos, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_u_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_v_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_pwat, how = 'left', on = 'coord')\n",
    "    dateval = min(dataf.date.unique())\n",
    "    agg_data['date'] = dateval\n",
    "    \n",
    "\n",
    "\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data and create a df for each day\n",
    "\n",
    "def downloadData(file_list, dayDF2, cnt, skip):\n",
    "    count1 = 0\n",
    "\n",
    "    dayDF = None\n",
    "    # dayDF2_main = None\n",
    "    p = False\n",
    "    for file in file_list:\n",
    "        filename = dspath + file\n",
    "        outfile = save_dir + os.path.basename(filename)\n",
    "        print('Trying to downloading', file)\n",
    "        req = requests.get(filename, cookies = ret.cookies, allow_redirects=True)\n",
    "        if req.status_code != 200:\n",
    "            print('File not found')\n",
    "            continue\n",
    "        else:\n",
    "            count1+=1\n",
    "            open(outfile, 'wb').write(req.content)\n",
    "            file_name = save_dir + file.split('/')[-1]\n",
    "            print('downloaded')\n",
    "            mainDF = getData(file.split('/')[-1])\n",
    "            \n",
    "            if dayDF is None:\n",
    "                dayDF = mainDF.copy(deep = True)\n",
    "            else:\n",
    "                dayDF = pd.concat([dayDF, mainDF])\n",
    "            \n",
    "            if count1>1:\n",
    "                if cnt == 0:\n",
    "                    dayDF2 = mainDF.copy(deep = True)\n",
    "                else:\n",
    "                    dayDF2 = pd.concat([dayDF2, mainDF])\n",
    "                cnt+=1\n",
    "            if count1 == 1 and cnt ==3:\n",
    "                dayDF2 = pd.concat([dayDF2, mainDF])\n",
    "                dayDF2_main = dayDF2.copy(deep = True)\n",
    "                dayDF2 = None\n",
    "                p = True\n",
    "                cnt=0\n",
    "                \n",
    "    if dayDF is None:\n",
    "        return None, None, 0, None\n",
    "\n",
    "    aggregated = aggregateData9am(dayDF)\n",
    "    if p:\n",
    "        # print(dayDF2_main)\n",
    "        # print(min(dayDF2_main.date.unique()))\n",
    "        aggregated_4pm = aggregateData4pm(dayDF2_main)\n",
    "        return aggregated, dayDF2, cnt, aggregated_4pm\n",
    "        # return aggregated, dayDF2, cnt, None\n",
    "\n",
    "    return aggregated, dayDF2, cnt, None\n",
    "    # return dayDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to downloading grib2/2016/2016.02/fnl_20160210_00_00.grib2\n",
      "downloaded\n",
      "Trying to downloading grib2/2016/2016.02/fnl_20160210_06_00.grib2\n",
      "downloaded\n",
      "Trying to downloading grib2/2016/2016.02/fnl_20160210_12_00.grib2\n",
      "downloaded\n",
      "Trying to downloading grib2/2016/2016.02/fnl_20160210_18_00.grib2\n",
      "downloaded\n"
     ]
    }
   ],
   "source": [
    "# ['grib2/2020/2020.01/fnl_20200101_12_00.grib2']\n",
    "cnt_4pm = 0\n",
    "df_4pm = None\n",
    "mainDF_4pm = None\n",
    "# mainfiles = [['grib2/2019/2019.12/fnl_20191218_00_00.grib2','grib2/2019/2019.12/fnl_20191218_06_00.grib2','grib2/2019/2019.12/fnl_20191218_12_00.grib2','grib2/2021/2021.12/fnl_20211231_18_00.grib2'],['grib2/2022/2022.01/fnl_20220101_00_00.grib2']]\n",
    "mainfiles = [['grib2/2016/2016.02/fnl_20160210_00_00.grib2','grib2/2016/2016.02/fnl_20160210_06_00.grib2','grib2/2016/2016.02/fnl_20160210_12_00.grib2', 'grib2/2016/2016.02/fnl_20160210_18_00.grib2']]\n",
    "for filelist in mainfiles:\n",
    "                \n",
    "            # print(filelist)\n",
    "    mainDF, df_4pm, cnt_4pm, mainDF_4pm= downloadData(filelist, df_4pm, cnt_4pm, skip = False)\n",
    "    \n",
    "    if mainDF is not None:\n",
    "        if mainDF_4pm is None:\n",
    "            mainDF_9am = mainDF.copy(deep = True)\n",
    "        else:\n",
    "            all_data = mainDF_9am.merge(mainDF_4pm, how = 'left', on = 'coord')\n",
    "            all_data = all_data.drop(['date_x'], axis = 1)\n",
    "            all_data = all_data.rename(columns = {'date_y':'date'})\n",
    "            # print(all_data.columns)\n",
    "            # addToDb(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/pollensource/gfs_archive\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in os.listdir():\n",
    "    data = getData(file)\n",
    "    addToDb(data.drop(['date'], axis = 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "databse = 'gfs_data'\n",
    "user = 'gfs'\n",
    "password = 'pollenmelb'\n",
    "host=\"127.0.0.1\"\n",
    "\n",
    "conn_string = f\"postgresql://{user}:{password}@{host}/{databse}\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbConnection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select * from \\\"weatherraw\\\" where coord[0] = 145.0 and coord[1] = -38\", dbConnection)# pd.set_option('display.expand_frame_repr', False);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = x.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4110784/2499121826.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data.drop(['date'], axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "data.drop(['date'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"all_data_raw.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex([], dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

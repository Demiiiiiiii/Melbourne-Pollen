{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database to Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "databse = 'gfs_data'\n",
    "user = 'gfs'\n",
    "password = 'pollenmelb'\n",
    "host=\"127.0.0.1\"\n",
    "\n",
    "conn_string = f\"postgresql://{user}:{password}@{host}/{databse}\"\n",
    "engine = create_engine(conn_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbConnection = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select * from \\\"weatherraw\\\" where coord[0] = 145.0 and coord[1] = -38\", dbConnection)# pd.set_option('display.expand_frame_repr', False);\n",
    "x= df.sort_values(by = 'datetime')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>t</th>\n",
       "      <th>t_2m</th>\n",
       "      <th>msl</th>\n",
       "      <th>u_10m</th>\n",
       "      <th>v_10m</th>\n",
       "      <th>hum_atmos</th>\n",
       "      <th>pwat</th>\n",
       "      <th>coord</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>694</th>\n",
       "      <td>2000-01-01 00:00:00</td>\n",
       "      <td>288.6</td>\n",
       "      <td>288.3</td>\n",
       "      <td>101740.0</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>32.0</td>\n",
       "      <td>12.2</td>\n",
       "      <td>(145,-38)</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8643</th>\n",
       "      <td>2000-01-01 06:00:00</td>\n",
       "      <td>290.8</td>\n",
       "      <td>290.3</td>\n",
       "      <td>101654.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>31.0</td>\n",
       "      <td>14.3</td>\n",
       "      <td>(145,-38)</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7466</th>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "      <td>284.0</td>\n",
       "      <td>284.0</td>\n",
       "      <td>101903.0</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>3.400000</td>\n",
       "      <td>29.0</td>\n",
       "      <td>13.3</td>\n",
       "      <td>(145,-38)</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>2000-01-01 18:00:00</td>\n",
       "      <td>280.1</td>\n",
       "      <td>280.4</td>\n",
       "      <td>101806.0</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>2.300000</td>\n",
       "      <td>33.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>(145,-38)</td>\n",
       "      <td>2000-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8828</th>\n",
       "      <td>2000-01-02 00:00:00</td>\n",
       "      <td>292.3</td>\n",
       "      <td>291.4</td>\n",
       "      <td>101842.0</td>\n",
       "      <td>-1.1</td>\n",
       "      <td>1.499999</td>\n",
       "      <td>28.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>(145,-38)</td>\n",
       "      <td>2000-01-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                datetime      t   t_2m       msl  u_10m     v_10m  hum_atmos  \\\n",
       "694  2000-01-01 00:00:00  288.6  288.3  101740.0   -1.6  3.500000       32.0   \n",
       "8643 2000-01-01 06:00:00  290.8  290.3  101654.0   -0.1  6.000000       31.0   \n",
       "7466 2000-01-01 12:00:00  284.0  284.0  101903.0   -1.2  3.400000       29.0   \n",
       "3150 2000-01-01 18:00:00  280.1  280.4  101806.0   -0.9  2.300000       33.0   \n",
       "8828 2000-01-02 00:00:00  292.3  291.4  101842.0   -1.1  1.499999       28.0   \n",
       "\n",
       "      pwat      coord        date  \n",
       "694   12.2  (145,-38)  2000-01-01  \n",
       "8643  14.3  (145,-38)  2000-01-01  \n",
       "7466  13.3  (145,-38)  2000-01-01  \n",
       "3150  14.9  (145,-38)  2000-01-01  \n",
       "8828  13.0  (145,-38)  2000-01-02  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add missing vals to db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "mainfiles = []\n",
    "for date in missing:\n",
    "    files = []\n",
    "    vals = date.split('-')\n",
    "    if int(vals[0])<2008:\n",
    "        grbval = 'grib1'\n",
    "    else:\n",
    "        grbval = 'grib2'\n",
    "    date1 = '{}/{}/{}.{}/fnl_{}_00_00.{}'.format(grbval, vals[0], vals[0], vals[1], vals[0]+vals[1]+vals[2], grbval)\n",
    "    date2 = '{}/{}/{}.{}/fnl_{}_06_00.{}'.format(grbval, vals[0], vals[0], vals[1], vals[0]+vals[1]+vals[2], grbval)\n",
    "    date3 = '{}/{}/{}.{}/fnl_{}_12_00.{}'.format(grbval, vals[0], vals[0], vals[1], vals[0]+vals[1]+vals[2], grbval)\n",
    "    date4 = '{}/{}/{}.{}/fnl_{}_18_00.{}'.format(grbval, vals[0], vals[0], vals[1], vals[0]+vals[1]+vals[2], grbval)\n",
    "    if vals[1] == '12':\n",
    "        date5 = '{}/{}/{}.{}/fnl_{}_00_00.{}'.format(grbval, vals[0], vals[0], vals[1], vals[0]+vals[1]+str(int(vals[2])+1), grbval)\n",
    "    else:\n",
    "        date5 = '{}/{}/{}.{}/fnl_{}_00_00.{}'.format(grbval, vals[0], vals[0], str(int(vals[1])%12 + 1).zfill(2), vals[0]+str(int(vals[1])%12 + 1).zfill(2)+'01', grbval)\n",
    "    files.append(date1)\n",
    "    files.append(date2)\n",
    "    files.append(date3)\n",
    "    files.append(date4)\n",
    "    files.append(date5)\n",
    "    mainfiles.append(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pygrib\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import colors\n",
    "import numpy as np\n",
    "from mpl_toolkits.basemap import Basemap, addcyclic\n",
    "from datetime import datetime\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup to download grib files from RDA archive\n",
    "\n",
    "try:\n",
    "    import getpass\n",
    "    input = getpass.getpass\n",
    "except:\n",
    "    try:\n",
    "        input = raw_input\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user specific data to login\n",
    "\n",
    "pswd = input('password:')\n",
    "values = {'email' : 'nandlalm@student.unimelb.edu.au', 'passwd' : pswd, 'action' : 'login'}\n",
    "login_url = 'https://rda.ucar.edu/cgi-bin/login'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = requests.post(login_url, data=values)\n",
    "if ret.status_code != 200:\n",
    "    print('Bad Authentication')\n",
    "    print(ret.text)\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file path on RDA catalogue\n",
    "\n",
    "dspath = 'https://rda.ucar.edu/data/ds083.2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# destinantion path\n",
    "\n",
    "save_dir = '/pollensource/gfs_archive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bounding box for Australia\n",
    "\n",
    "min_lat = -43.835\n",
    "max_lat = -9.796    \n",
    "min_lon = 112.500\n",
    "max_lon = 154.688\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAus(file):\n",
    "    lat_filter = (file[\"lat\"] >= min_lat) & (file[\"lat\"] <= max_lat)\n",
    "    lon_filter = (file[\"lon\"] >= min_lon) & (file[\"lon\"] <= max_lon)\n",
    "    filtered = file.loc[lat_filter & lon_filter]\n",
    "    return filtered.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract relevant data from the grib files\n",
    "\n",
    "def getData(filename):\n",
    "\n",
    "    fileAddress = '/pollensource/gfs_archive/' + filename\n",
    "    myfile = pygrib.open(fileAddress)\n",
    "\n",
    "    # get surface level temperature data\n",
    "    grb_temp = myfile.select(shortName='t', typeOfLevel = 'surface')\n",
    "    lat = grb_temp[0].latitudes\n",
    "    lon = grb_temp[0].longitudes\n",
    "    vals = grb_temp[0].values.reshape(lon.shape[0],)\n",
    "    date = grb_temp[0].validDate\n",
    "    \n",
    "    df = pd.DataFrame(None, index=range(65160), columns=['datetime', 'lat', 'lon', 't'])  # range hard-coded for now\n",
    "    df['datetime'] = date\n",
    "    df['lat'] = lat\n",
    "    df['lon'] = lon\n",
    "    df['t'] = vals\n",
    "    df['date'] = pd.to_datetime(df['datetime']).dt.date\n",
    "    \n",
    "    # get 2m level temperature data\n",
    "    grb_temp_2m = myfile.select(shortName='2t')\n",
    "    vals_2t = grb_temp_2m[0].values.reshape(lon.shape[0],)\n",
    "    df['t_2m'] = vals_2t\n",
    "\n",
    "    # get mean sea level pressure\n",
    "    try:\n",
    "        grb_msl = myfile.select(shortName='msl')\n",
    "    except ValueError:\n",
    "        grb_msl = myfile.select(shortName='prmsl')\n",
    "    vals_msl = grb_msl[0].values.reshape(lon.shape[0],)\n",
    "    df['msl'] = vals_msl\n",
    "\n",
    "    # get 10m u component of wind\n",
    "    grb_10u = myfile.select(shortName='10u')\n",
    "    vals_10u = grb_10u[0].values.reshape(lon.shape[0],)\n",
    "    df['u_10m'] = vals_10u\n",
    "\n",
    "    # get 10m v component of wind\n",
    "    grb_10v = myfile.select(shortName='10v')\n",
    "    vals_10v = grb_10v[0].values.reshape(lon.shape[0],)\n",
    "    df['v_10m'] = vals_10v\n",
    "\n",
    "    # get the relative humidity\n",
    "    try:\n",
    "        grb_r = myfile.select(shortName='r', typeOfLevel = 'entireAtmosphere')\n",
    "    except ValueError:\n",
    "        grb_r = myfile.select(shortName='r', typeOfLevel = 'atmosphereSingleLayer')\n",
    "\n",
    "    vals_r = grb_r[0].values.reshape(lon.shape[0],)\n",
    "    df['hum_atmos'] = vals_r\n",
    "\n",
    "    # get precipitable water\n",
    "    grb_pwat = myfile.select(shortName='pwat')\n",
    "    vals_pwat = grb_pwat[0].values.reshape(lon.shape[0],)\n",
    "    df['pwat'] = vals_pwat\n",
    "\n",
    "    data  = filterAus(df)\n",
    "    # print(data)\n",
    "    # data['coord'] = (data['lon'], data['lat'])\n",
    "    data['coord'] = list(zip(data.lon, data.lat))\n",
    "    data.coord = data.coord.astype(str)\n",
    "\n",
    "    data = data.drop(['lat', 'lon'], axis = 1)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def addToDb(df):\n",
    "\n",
    "    databse = 'gfs_data'\n",
    "    user = 'gfs'\n",
    "    password = 'pollenmelb'\n",
    "    host=\"127.0.0.1\"\n",
    "\n",
    "    conn_string = f\"postgresql://{user}:{password}@{host}/{databse}\"\n",
    "    engine = create_engine(conn_string)\n",
    "    \n",
    "    table_name = 'weather'\n",
    "    \n",
    "    if_exists = 'append'\n",
    "\n",
    "    #Write the data to postgres\n",
    "\n",
    "    with engine.connect() as con:\n",
    "        df.to_sql(\n",
    "            name=table_name.lower(), \n",
    "            con=con, \n",
    "            if_exists=if_exists,\n",
    "            index = False\n",
    "        )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the hourly data and create variables\n",
    "\n",
    "def aggregateData9am(dataf):\n",
    "\n",
    "    grouped_t = dataf.groupby(['coord']).agg({'t': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t.columns = ['t_mean_9am', 't_min_9am', 't_max_9am', 't_sd_9am']\n",
    "    grouped_t = grouped_t.reset_index()\n",
    "\n",
    "    grouped_t_2m = dataf.groupby(['coord']).agg({'t_2m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t_2m.columns = ['t_2m_mean_9am', 't_2m_min_9am', 't_2m_max_9am', 't_2m_sd_9am']\n",
    "    grouped_t_2m = grouped_t_2m.reset_index()\n",
    "    \n",
    "    grouped_msl = dataf.groupby(['coord']).agg({'msl': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_msl.columns = ['msl_mean_9am', 'msl_min_9am', 'msl_max_9am', 'msl_sd_9am']\n",
    "    grouped_msl = grouped_msl.reset_index()\n",
    "\n",
    "    grouped_hum_atmos = dataf.groupby(['coord']).agg({'hum_atmos': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_hum_atmos.columns = ['hum_atmos_mean_9am', 'hum_atmos_min_9am', 'hum_atmos_max_9am', 'hum_atmos_sd_9am']\n",
    "    grouped_hum_atmos = grouped_hum_atmos.reset_index()\n",
    "\n",
    "    grouped_u_10m = dataf.groupby(['coord']).agg({'u_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_u_10m.columns = ['u_10m_mean_9am', 'u_10m_min_9am', 'u_10m_max_9am', 'u_10m_sd_9am']\n",
    "    grouped_u_10m = grouped_u_10m.reset_index()\n",
    "\n",
    "    grouped_v_10m = dataf.groupby(['coord']).agg({'v_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_v_10m.columns = ['v_10m_mean_9am', 'v_10m_min_9am', 'v_10m_max_9am', 'v_10m_sd_9am']\n",
    "    grouped_v_10m = grouped_v_10m.reset_index()\n",
    "\n",
    "    grouped_pwat = dataf.groupby(['coord']).agg({'pwat': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_pwat.columns = ['pwat_mean_9am', 'pwat_min_9am', 'pwat_max_9am', 'pwat_sd_9am']\n",
    "    grouped_pwat = grouped_pwat.reset_index()\n",
    "\n",
    "    agg_data = grouped_t.merge(grouped_t_2m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_msl, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_hum_atmos, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_u_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_v_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_pwat, how = 'left', on = 'coord')\n",
    "\n",
    "    dateval = min(dataf.date.unique())\n",
    "    agg_data['date'] = dateval\n",
    "    \n",
    "\n",
    "\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregate the hourly data and create variables\n",
    "\n",
    "def aggregateData4pm(dataf):\n",
    "\n",
    "    grouped_t = dataf.groupby(['coord']).agg({'t': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t.columns = ['t_mean_4pm', 't_min_4pm', 't_max_4pm', 't_sd_4pm']\n",
    "    grouped_t = grouped_t.reset_index()\n",
    "\n",
    "    grouped_t_2m = dataf.groupby(['coord']).agg({'t_2m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_t_2m.columns = ['t_2m_mean_4pm', 't_2m_min_4pm', 't_2m_max_4pm', 't_2m_sd_4pm']\n",
    "    grouped_t_2m = grouped_t_2m.reset_index()\n",
    "\n",
    "    grouped_msl = dataf.groupby(['coord']).agg({'msl': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_msl.columns = ['msl_mean_4pm', 'msl_min_4pm', 'msl_max_4pm', 'msl_sd_4pm']\n",
    "    grouped_msl = grouped_msl.reset_index()\n",
    "\n",
    "    grouped_hum_atmos = dataf.groupby(['coord']).agg({'hum_atmos': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_hum_atmos.columns = ['hum_atmos_mean_4pm', 'hum_atmos_min_4pm', 'hum_atmos_max_4pm', 'hum_atmos_sd_4pm']\n",
    "    grouped_hum_atmos = grouped_hum_atmos.reset_index()\n",
    "\n",
    "    grouped_u_10m = dataf.groupby(['coord']).agg({'u_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_u_10m.columns = ['u_10m_mean_4pm', 'u_10m_min_4pm', 'u_10m_max_4pm', 'u_10m_sd_4pm']\n",
    "    grouped_u_10m = grouped_u_10m.reset_index()\n",
    "\n",
    "    grouped_v_10m = dataf.groupby(['coord']).agg({'v_10m': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_v_10m.columns = ['v_10m_mean_4pm', 'v_10m_min_4pm', 'v_10m_max_4pm', 'v_10m_sd_4pm']\n",
    "    grouped_v_10m = grouped_v_10m.reset_index()\n",
    "\n",
    "    grouped_pwat = dataf.groupby(['coord']).agg({'pwat': ['mean', 'min', 'max', np.std]})\n",
    "    grouped_pwat.columns = ['pwat_mean_4pm', 'pwat_min_4pm', 'pwat_max_4pm', 'pwat_sd_4pm']\n",
    "    grouped_pwat = grouped_pwat.reset_index()\n",
    "\n",
    "\n",
    "    agg_data = grouped_t.merge(grouped_t_2m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_msl, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_hum_atmos, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_u_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_v_10m, how = 'left', on = 'coord')\n",
    "    agg_data = agg_data.merge(grouped_pwat, how = 'left', on = 'coord')\n",
    "    dateval = min(dataf.date.unique())\n",
    "    agg_data['date'] = dateval\n",
    "    \n",
    "\n",
    "\n",
    "    return agg_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data and create a df for each day\n",
    "\n",
    "def downloadData(file_list, dayDF2, cnt, skip):\n",
    "    count1 = 0\n",
    "\n",
    "    dayDF = None\n",
    "    # dayDF2_main = None\n",
    "    p = False\n",
    "    for file in file_list:\n",
    "        filename = dspath + file\n",
    "        outfile = save_dir + os.path.basename(filename)\n",
    "        print('Trying to downloading', file)\n",
    "        req = requests.get(filename, cookies = ret.cookies, allow_redirects=True)\n",
    "        if req.status_code != 200:\n",
    "            print('File not found')\n",
    "            continue\n",
    "        else:\n",
    "            count1+=1\n",
    "            open(outfile, 'wb').write(req.content)\n",
    "            file_name = save_dir + file.split('/')[-1]\n",
    "            print('downloaded')\n",
    "            mainDF = getData(file.split('/')[-1])\n",
    "            \n",
    "            if dayDF is None:\n",
    "                dayDF = mainDF.copy(deep = True)\n",
    "            else:\n",
    "                dayDF = pd.concat([dayDF, mainDF])\n",
    "            \n",
    "            if count1>1:\n",
    "                if cnt == 0:\n",
    "                    dayDF2 = mainDF.copy(deep = True)\n",
    "                else:\n",
    "                    dayDF2 = pd.concat([dayDF2, mainDF])\n",
    "                cnt+=1\n",
    "            if count1 == 1 and cnt ==3:\n",
    "                dayDF2 = pd.concat([dayDF2, mainDF])\n",
    "                dayDF2_main = dayDF2.copy(deep = True)\n",
    "                dayDF2 = None\n",
    "                p = True\n",
    "                cnt=0\n",
    "                \n",
    "    if dayDF is None:\n",
    "        return None, None, 0, None\n",
    "\n",
    "    aggregated = aggregateData9am(dayDF)\n",
    "    if p:\n",
    "        # print(dayDF2_main)\n",
    "        # print(min(dayDF2_main.date.unique()))\n",
    "        aggregated_4pm = aggregateData4pm(dayDF2_main)\n",
    "        return aggregated, dayDF2, cnt, aggregated_4pm\n",
    "        # return aggregated, dayDF2, cnt, None\n",
    "\n",
    "    return aggregated, dayDF2, cnt, None\n",
    "    # return dayDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to downloading grib1/2007/2007.02/fnl_20070228_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.02/fnl_20070228_06_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.02/fnl_20070228_12_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.02/fnl_20070228_18_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.03/fnl_20070301_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.04/fnl_20070430_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.04/fnl_20070430_06_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.04/fnl_20070430_12_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.04/fnl_20070430_18_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.05/fnl_20070501_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.06/fnl_20070630_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.06/fnl_20070630_06_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.06/fnl_20070630_12_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.06/fnl_20070630_18_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.07/fnl_20070701_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.09/fnl_20070930_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.09/fnl_20070930_06_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.09/fnl_20070930_12_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.09/fnl_20070930_18_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.10/fnl_20071001_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.11/fnl_20071130_00_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.11/fnl_20071130_06_00.grib1\n",
      "downloaded\n",
      "Trying to downloading grib1/2007/2007.11/fnl_20071130_12_00.grib1\n",
      "downloaded\n"
     ]
    }
   ],
   "source": [
    "# ['grib2/2020/2020.01/fnl_20200101_12_00.grib2']\n",
    "cnt_4pm = 0\n",
    "df_4pm = None\n",
    "mainDF_4pm = None\n",
    "\n",
    "for filelist in mainfiles[35:]:\n",
    "                \n",
    "            # print(filelist)\n",
    "    mainDF, df_4pm, cnt_4pm, mainDF_4pm= downloadData(filelist, df_4pm, cnt_4pm, skip = False)\n",
    "    \n",
    "    if mainDF is not None:\n",
    "        if mainDF_4pm is None:\n",
    "            mainDF_9am = mainDF.copy(deep = True)\n",
    "        else:\n",
    "            all_data = mainDF_9am.merge(mainDF_4pm, how = 'left', on = 'coord')\n",
    "            all_data = all_data.drop(['date_x'], axis = 1)\n",
    "            all_data = all_data.rename(columns = {'date_y':'date'})\n",
    "            # print(all_data.columns)\n",
    "            addToDb(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grib1/2007/2007.02/fnl_20070228_00_00.grib1',\n",
       " 'grib1/2007/2007.02/fnl_20070228_06_00.grib1',\n",
       " 'grib1/2007/2007.02/fnl_20070228_12_00.grib1',\n",
       " 'grib1/2007/2007.02/fnl_20070228_18_00.grib1',\n",
       " 'grib1/2007/2007.03/fnl_20070301_00_00.grib1']"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mainfiles[35]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
